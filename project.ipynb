{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c78ecbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.12/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/emil/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/emil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/emil/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25221d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_names = [\"oracc_cams\", \"oracc_dcclt\", \"oracc_ribo\", \"oracc_rinap\", \"oracc_saao\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8e7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 19370 lines\n",
      "oracc_dcclt: 0 lines\n",
      "oracc_ribo: 1027 lines\n",
      "oracc_rinap: 2434 lines\n",
      "oracc_saao: 40064 lines\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenized_corpora = {}\n",
    "for corpus_name in corpus_names:\n",
    "    with open(f\"parsed_dataset/translations_{corpus_name}.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"{corpus_name}: {len(lines)} lines\")\n",
    "        lines = [nltk.word_tokenize(line) for line in lines]\n",
    "        tokenized_corpora[corpus_name] = lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03774f",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b74fceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 322766 tokens\n",
      "oracc_dcclt: 0 tokens\n",
      "oracc_ribo: 50345 tokens\n",
      "oracc_rinap: 173278 tokens\n",
      "oracc_saao: 618420 tokens\n"
     ]
    }
   ],
   "source": [
    "# extract tokens into a flat list for each corpus\n",
    "flat_token_lists = {}\n",
    "for corpus_name in corpus_names:\n",
    "    flat_tokens = []\n",
    "    for line in tokenized_corpora[corpus_name]:\n",
    "        flat_tokens.extend(line)\n",
    "    flat_token_lists[corpus_name] = flat_tokens\n",
    "\n",
    "for corpus_name in corpus_names:\n",
    "    num_tokens = len(flat_token_lists[corpus_name])\n",
    "    print(f\"{corpus_name}: {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a200f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def exists_in_wordnet(word):\n",
    "    return bool(wordnet.synsets(word))\n",
    "\n",
    "def exists_in_stopwords(word):\n",
    "    return word.lower() in stop_words\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return word not in [\",\", \".\", \";\", \":\", \"!\", \"?\", \"'\", '\"', \"-\", \"—\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"...\", \"`\", \"``\", \"''\", \"“\", \"”\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63d9fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'son', 'of', 'the', 'king', 'of', 'the', 'inhabited', 'world', 'the', 'resplendent', 'one', 'the', 'beloved', 'of', 'mami', 'let', 'me', 'sing', 'of', 'the', 'mighty', 'one', 'the', 'son', 'of', 'enlil', 'ninurta', 'the', 'resplendent', 'one', 'the', 'beloved', 'of', 'mami', 'let', 'me', 'praise', 'the', 'mighty', 'one', 'the', 'god', 'the', 'son', 'of', 'enlil', 'the', 'offspring', 'of']\n"
     ]
    }
   ],
   "source": [
    "stopword_removed_corpora = {}\n",
    "for corpus_name in corpus_names:\n",
    "    line = flat_token_lists[corpus_name]\n",
    "    filtered_line = []\n",
    "    for word in line:\n",
    "        if is_valid_word(word):\n",
    "            filtered_line.append(word.lower())\n",
    "    stopword_removed_corpora[corpus_name] = filtered_line\n",
    "\n",
    "print(stopword_removed_corpora[\"oracc_cams\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5369ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'son', 'of', 'the', 'king']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "stemmed_corpora = {}\n",
    "for corpus_name in corpus_names:\n",
    "    lines = stopword_removed_corpora[corpus_name]\n",
    "    stemmed_lines = stem_words(lines)\n",
    "    stemmed_corpora[corpus_name] = stemmed_lines\n",
    "\n",
    "# test print\n",
    "print(stemmed_corpora[\"oracc_cams\"][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7fc124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 235673 stemmed tokens\n",
      "oracc_dcclt: 0 stemmed tokens\n",
      "oracc_ribo: 37627 stemmed tokens\n",
      "oracc_rinap: 129354 stemmed tokens\n",
      "oracc_saao: 484901 stemmed tokens\n"
     ]
    }
   ],
   "source": [
    "# calculate total number of tokens in each stemmed corpus\n",
    "num_stemmed_tokens = {}\n",
    "for corpus_name in corpus_names:\n",
    "    num_tokens = len(stemmed_corpora[corpus_name])\n",
    "    num_stemmed_tokens[corpus_name] = num_tokens\n",
    "    print(f\"{corpus_name}: {num_tokens} stemmed tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1db18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
