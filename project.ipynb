{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c78ecbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.12/site-packages (3.9.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.12/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-2.3.4 pandas-2.3.3 pytz-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/emil/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/emil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/emil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk pandas\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c25221d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_names = [\"oracc_cams\", \"oracc_ribo\", \"oracc_rinap\", \"oracc_saao\"]\n",
    "# corpus oracc_dcclt has been excluded due to it having no valid translations.\n",
    "corpus_display_names = {\n",
    "    \"oracc_cams\": \"Corpus of Ancient Mesopotamian Scholarship\",\n",
    "    \"oracc_ribo\": \"Royal Inscriptions of Babylonia online\",\n",
    "    \"oracc_rinap\": \"Royal Inscriptions of the Neo-Assyrian Period\",\n",
    "    \"oracc_saao\": \"State Archives of Assyria Online\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b3b8e7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 19370 lines\n",
      "oracc_ribo: 1027 lines\n",
      "oracc_rinap: 2434 lines\n",
      "oracc_saao: 40064 lines\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenized_corpora = {}\n",
    "for corpus_name in corpus_names:\n",
    "    with open(f\"parsed_dataset/translations_{corpus_name}.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"{corpus_name}: {len(lines)} lines\")\n",
    "        lines = [nltk.word_tokenize(line) for line in lines]\n",
    "        tokenized_corpora[corpus_name] = lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03774f",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b74fceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 322766 tokens\n",
      "oracc_ribo: 50345 tokens\n",
      "oracc_rinap: 173278 tokens\n",
      "oracc_saao: 618420 tokens\n"
     ]
    }
   ],
   "source": [
    "# extract tokens into a flat list for each corpus\n",
    "flat_token_lists = {}\n",
    "for corpus_name in corpus_names:\n",
    "    flat_tokens = []\n",
    "    for line in tokenized_corpora[corpus_name]:\n",
    "        flat_tokens.extend(line)\n",
    "    flat_token_lists[corpus_name] = flat_tokens\n",
    "\n",
    "for corpus_name in corpus_names:\n",
    "    num_tokens = len(flat_token_lists[corpus_name])\n",
    "    print(f\"{corpus_name}: {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a200f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_word(word):\n",
    "    return word not in [\",\", \".\", \";\", \":\", \"!\", \"?\", \"'\", '\"', \"-\", \"—\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"...\", \"`\", \"``\", \"''\", \"“\", \"”\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63d9fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'son', 'of', 'the', 'king', 'of', 'the', 'inhabited', 'world', 'the', 'resplendent', 'one', 'the', 'beloved', 'of', 'mami', 'let', 'me', 'sing', 'of', 'the', 'mighty', 'one', 'the', 'son', 'of', 'enlil', 'ninurta', 'the', 'resplendent', 'one', 'the', 'beloved', 'of', 'mami', 'let', 'me', 'praise', 'the', 'mighty', 'one', 'the', 'god', 'the', 'son', 'of', 'enlil', 'the', 'offspring', 'of']\n"
     ]
    }
   ],
   "source": [
    "stopword_removed_corpora = {}\n",
    "for corpus_name in corpus_names:\n",
    "    line = flat_token_lists[corpus_name]\n",
    "    filtered_line = []\n",
    "    for word in line:\n",
    "        if is_valid_word(word):\n",
    "            filtered_line.append(word.lower())\n",
    "    stopword_removed_corpora[corpus_name] = filtered_line\n",
    "\n",
    "print(stopword_removed_corpora[\"oracc_cams\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5369ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'son', 'of', 'the', 'king']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "stemmed_corpora = {}\n",
    "for corpus_name in corpus_names:\n",
    "    lines = stopword_removed_corpora[corpus_name]\n",
    "    stemmed_lines = stem_words(lines)\n",
    "    stemmed_corpora[corpus_name] = stemmed_lines\n",
    "\n",
    "# test print\n",
    "print(stemmed_corpora[\"oracc_cams\"][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7fc124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 235673 stemmed tokens\n",
      "oracc_ribo: 37627 stemmed tokens\n",
      "oracc_rinap: 129354 stemmed tokens\n",
      "oracc_saao: 484901 stemmed tokens\n"
     ]
    }
   ],
   "source": [
    "# calculate total number of tokens in each stemmed corpus\n",
    "num_stemmed_tokens = {}\n",
    "for corpus_name in corpus_names:\n",
    "    num_tokens = len(stemmed_corpora[corpus_name])\n",
    "    num_stemmed_tokens[corpus_name] = num_tokens\n",
    "    print(f\"{corpus_name}: {num_tokens} stemmed tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68d1db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def exists_in_wordnet(word):\n",
    "    return bool(wordnet.synsets(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b15de157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: 0.5712 proportion of tokens in WordNet\n",
      "oracc_ribo: 0.5377 proportion of tokens in WordNet\n",
      "oracc_rinap: 0.5087 proportion of tokens in WordNet\n",
      "oracc_saao: 0.5351 proportion of tokens in WordNet\n"
     ]
    }
   ],
   "source": [
    "# calculate proportion of tokens that exist in WordNet for each corpus\n",
    "wordnet_proportions = {}\n",
    "for corpus_name in corpus_names:\n",
    "    stemmed_tokens = stemmed_corpora[corpus_name]\n",
    "    if len(stemmed_tokens) > 0:\n",
    "        count_in_wordnet = sum(1 for token in stemmed_tokens if exists_in_wordnet(token))\n",
    "        proportion = count_in_wordnet / len(stemmed_tokens)\n",
    "        wordnet_proportions[corpus_name] = proportion\n",
    "        print(f\"{corpus_name}: {proportion:.4f} proportion of tokens in WordNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d05876ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracc_cams: vocabulary size: 8051\n",
      "oracc_ribo: vocabulary size: 2389\n",
      "oracc_rinap: vocabulary size: 4717\n",
      "oracc_saao: vocabulary size: 13192\n"
     ]
    }
   ],
   "source": [
    "# calculate vocabulary sizes\n",
    "vocabulary_sizes = {}\n",
    "for corpus_name in corpus_names:\n",
    "    unique_tokens = set(stemmed_corpora[corpus_name])\n",
    "    vocabulary_sizes[corpus_name] = len(unique_tokens)\n",
    "    print(f\"{corpus_name}: vocabulary size: {len(unique_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0769cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Total Tokens</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Proportion in WordNet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corpus of Ancient Mesopotamian Scholarship</td>\n",
       "      <td>235673</td>\n",
       "      <td>8051</td>\n",
       "      <td>0.571181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Royal Inscriptions of Babylonia online</td>\n",
       "      <td>37627</td>\n",
       "      <td>2389</td>\n",
       "      <td>0.537699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Royal Inscriptions of the Neo-Assyrian Period</td>\n",
       "      <td>129354</td>\n",
       "      <td>4717</td>\n",
       "      <td>0.508689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State Archives of Assyria Online</td>\n",
       "      <td>484901</td>\n",
       "      <td>13192</td>\n",
       "      <td>0.535111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Corpus  Total Tokens  \\\n",
       "0     Corpus of Ancient Mesopotamian Scholarship        235673   \n",
       "1         Royal Inscriptions of Babylonia online         37627   \n",
       "2  Royal Inscriptions of the Neo-Assyrian Period        129354   \n",
       "3               State Archives of Assyria Online        484901   \n",
       "\n",
       "   Vocabulary Size  Proportion in WordNet  \n",
       "0             8051               0.571181  \n",
       "1             2389               0.537699  \n",
       "2             4717               0.508689  \n",
       "3            13192               0.535111  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarizing findings in a table\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Corpus\": [corpus_display_names[name] for name in corpus_names],\n",
    "    \"Total Tokens\": [num_stemmed_tokens[name] for name in corpus_names],\n",
    "    \"Vocabulary Size\": [vocabulary_sizes[name] for name in corpus_names],\n",
    "    \"Proportion in WordNet\": [wordnet_proportions[name] for name in corpus_names]\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a474f6f",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "750b7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ukraine: Angry Zelensky vows to punish Russian atrocities', 'War in Ukraine: Taking cover in a town under attack', \"Ukraine war 'catastrophic for global food'\", \"Manchester Arena bombing: Saffie Roussos's parents on hearing the truth\", 'Ukraine conflict: Oil price soars to highest level since 2008']\n"
     ]
    }
   ],
   "source": [
    "# load BBC dataset with Pandas\n",
    "bbc_df = pd.read_csv(\"dataset/bbc_news.csv\")\n",
    "bbc_df.head()\n",
    "# extract title and description columns\n",
    "bbc_texts = bbc_df[\"title\"].tolist() + bbc_df[\"description\"].tolist()\n",
    "print(bbc_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9af1f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ukraine', ':', 'Angry', 'Zelensky', 'vows', 'to', 'punish', 'Russian', 'atrocities'], ['War', 'in', 'Ukraine', ':', 'Taking', 'cover', 'in', 'a', 'town', 'under', 'attack']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_bbc = [nltk.word_tokenize(text) for text in bbc_texts]\n",
    "print(tokenized_bbc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e85d01dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ukraine', ':', 'Angry', 'Zelensky', 'vows', 'to', 'punish', 'Russian', 'atrocities', 'War', 'in', 'Ukraine', ':', 'Taking', 'cover', 'in', 'a', 'town', 'under', 'attack', 'Ukraine', 'war', \"'catastrophic\", 'for', 'global', 'food', \"'\", 'Manchester', 'Arena', 'bombing', ':', 'Saffie', 'Roussos', \"'s\", 'parents', 'on', 'hearing', 'the', 'truth', 'Ukraine', 'conflict', ':', 'Oil', 'price', 'soars', 'to', 'highest', 'level', 'since', '2008']\n"
     ]
    }
   ],
   "source": [
    "# flatten the tokenized list\n",
    "flat_bbc_tokens = []\n",
    "for line in tokenized_bbc:\n",
    "    flat_bbc_tokens.extend(line)\n",
    "print(flat_bbc_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4cef84ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ukraine', 'angry', 'zelensky', 'vows', 'to', 'punish', 'russian', 'atrocities', 'war', 'in', 'ukraine', 'taking', 'cover', 'in', 'a', 'town', 'under', 'attack', 'ukraine', 'war', \"'catastrophic\", 'for', 'global', 'food', 'manchester', 'arena', 'bombing', 'saffie', 'roussos', \"'s\", 'parents', 'on', 'hearing', 'the', 'truth', 'ukraine', 'conflict', 'oil', 'price', 'soars', 'to', 'highest', 'level', 'since', '2008', 'ukraine', 'war', 'pm', 'to', 'hold']\n"
     ]
    }
   ],
   "source": [
    "# remove special characters and lowercase\n",
    "filtered_bbc_tokens = []\n",
    "for word in flat_bbc_tokens:\n",
    "    if is_valid_word(word):\n",
    "        filtered_bbc_tokens.append(word.lower())\n",
    "print(filtered_bbc_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1abff07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ukrain', 'angri', 'zelenski', 'vow', 'to', 'punish', 'russian', 'atroc', 'war', 'in', 'ukrain', 'take', 'cover', 'in', 'a', 'town', 'under', 'attack', 'ukrain', 'war', \"'catastroph\", 'for', 'global', 'food', 'manchest', 'arena', 'bomb', 'saffi', 'rousso', \"'s\", 'parent', 'on', 'hear', 'the', 'truth', 'ukrain', 'conflict', 'oil', 'price', 'soar', 'to', 'highest', 'level', 'sinc', '2008', 'ukrain', 'war', 'pm', 'to', 'hold']\n"
     ]
    }
   ],
   "source": [
    "stemmed_bbc_tokens = stem_words(filtered_bbc_tokens)\n",
    "print(stemmed_bbc_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5905e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Dataset: 1165809 tokens\n"
     ]
    }
   ],
   "source": [
    "# calculate total number of tokens in BBC dataset\n",
    "num_bbc_tokens = len(stemmed_bbc_tokens)\n",
    "print(f\"BBC Dataset: {num_bbc_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bb279ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Dataset: 0.6004 proportion of tokens in WordNet\n"
     ]
    }
   ],
   "source": [
    "# calculate proportion of tokens that exist in WordNet\n",
    "count_in_wordnet_bbc = sum(1 for token in stemmed_bbc_tokens if exists_in_wordnet(token))\n",
    "proportion_bbc = count_in_wordnet_bbc / len(stemmed_bbc_tokens)\n",
    "print(f\"BBC Dataset: {proportion_bbc:.4f} proportion of tokens in WordNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "46a86b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Dataset: vocabulary size: 33902\n"
     ]
    }
   ],
   "source": [
    "# calculate vocabulary size of BBC dataset\n",
    "unique_bbc_tokens = set(stemmed_bbc_tokens)\n",
    "vocabulary_size_bbc = len(unique_bbc_tokens)\n",
    "print(f\"BBC Dataset: vocabulary size: {vocabulary_size_bbc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2767b487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Total Tokens</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Proportion in WordNet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBC News Dataset</td>\n",
       "      <td>1165809</td>\n",
       "      <td>33902</td>\n",
       "      <td>0.600399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Corpus  Total Tokens  Vocabulary Size  Proportion in WordNet\n",
       "0  BBC News Dataset       1165809            33902               0.600399"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize results in a table\n",
    "bbc_data = {\n",
    "    \"Corpus\": [\"BBC News Dataset\"],\n",
    "    \"Total Tokens\": [num_bbc_tokens],\n",
    "    \"Vocabulary Size\": [vocabulary_size_bbc],\n",
    "    \"Proportion in WordNet\": [proportion_bbc]\n",
    "}\n",
    "bbc_df = pd.DataFrame(bbc_data)\n",
    "bbc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25148d",
   "metadata": {},
   "source": [
    "Results from two previous stages summarized in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "be152348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Total Tokens</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Proportion in WordNet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corpus of Ancient Mesopotamian Scholarship</td>\n",
       "      <td>235673</td>\n",
       "      <td>8051</td>\n",
       "      <td>0.571181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Royal Inscriptions of Babylonia online</td>\n",
       "      <td>37627</td>\n",
       "      <td>2389</td>\n",
       "      <td>0.537699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Royal Inscriptions of the Neo-Assyrian Period</td>\n",
       "      <td>129354</td>\n",
       "      <td>4717</td>\n",
       "      <td>0.508689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State Archives of Assyria Online</td>\n",
       "      <td>484901</td>\n",
       "      <td>13192</td>\n",
       "      <td>0.535111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC News Dataset</td>\n",
       "      <td>1165809</td>\n",
       "      <td>33902</td>\n",
       "      <td>0.600399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Corpus  Total Tokens  \\\n",
       "0     Corpus of Ancient Mesopotamian Scholarship        235673   \n",
       "1         Royal Inscriptions of Babylonia online         37627   \n",
       "2  Royal Inscriptions of the Neo-Assyrian Period        129354   \n",
       "3               State Archives of Assyria Online        484901   \n",
       "4                               BBC News Dataset       1165809   \n",
       "\n",
       "   Vocabulary Size  Proportion in WordNet  \n",
       "0             8051               0.571181  \n",
       "1             2389               0.537699  \n",
       "2             4717               0.508689  \n",
       "3            13192               0.535111  \n",
       "4            33902               0.600399  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_df = pd.concat([df, bbc_df])\n",
    "# reset index for cleaner look\n",
    "overall_df.reset_index(drop=True, inplace=True)\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c614a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
